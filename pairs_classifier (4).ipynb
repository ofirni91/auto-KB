{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "pos_yago = pd.read_csv('pos_yago_7.csv')\n",
    "neg_yago = pd.read_csv('neg_yago_7.csv')\n",
    "\n",
    "pos_tac = pd.read_csv('pos_7.csv')\n",
    "neg_tac = pd.read_csv('neg_7.csv')\n",
    "\n",
    "pos_train = pos_tac\n",
    "neg_train = neg_tac\n",
    "\n",
    "pos_unseen = pos_yago\n",
    "neg_unseen = neg_yago\n",
    "\n",
    "# pos_train = pos_tac\n",
    "# neg_train = neg_tac\n",
    "\n",
    "# pos_unseen = pd.read_csv('zero_shot_pov_pairs.csv')\n",
    "# neg_unseen = pd.read_csv('zero_shot_neg_pairs.csv')\n",
    "\n",
    "# pos_train = pd.read_csv('zero_shot_pos_pairs_train.csv')\n",
    "# neg_train = pd.read_csv('zero_shot_neg_pairs_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_vec(str_word_vec):\n",
    "    str_word_vec = str_word_vec[12:-33]\n",
    "    str_word_vec = re.sub(r'\\[', '', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\]', '', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\n', ' ', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\s+', ' ', str_word_vec)\n",
    "    vec = np.fromstring(str_word_vec, dtype=float, sep=' ')\n",
    "    return vec\n",
    "\n",
    "def str_to_vec_rep(str_word_vec):\n",
    "    str_word_vec = str_word_vec[12:-33]\n",
    "    str_word_vec = re.sub(r'\\[', '', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\]', '', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\n', ' ', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\s+', ' ', str_word_vec)\n",
    "    vec = np.fromstring(str_word_vec, dtype=float, sep=' ')\n",
    "    return vec\n",
    "\n",
    "def str_to_vec_rep_old(str_word_vec):\n",
    "    str_word_vec = str_word_vec[79:-25]\n",
    "    str_word_vec = re.sub(r'\\[', '', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\]', '', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\n', ' ', str_word_vec)\n",
    "    str_word_vec = re.sub(r'\\s+', ' ', str_word_vec)\n",
    "    vec = np.fromstring(str_word_vec, dtype=float, sep=',')\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str_to_vec_rep(pos_train.iloc[0]['a_word_vec']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-8f90cbef799c>:16: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  vec = np.fromstring(str_word_vec, dtype=float, sep=' ')\n"
     ]
    }
   ],
   "source": [
    "final_vecs_pos_unseen = []\n",
    "final_vecs_neg_unseen = []\n",
    "# all_data = [pos, neg]\n",
    "pos_labels_unseen = []\n",
    "neg_labels_unseen = []\n",
    "y = 0\n",
    "# pos_labels = np.array(pos['same_entity'])\n",
    "# neg_labels = np.array(neg['same_entity'])\n",
    "\n",
    "for index, row in pos_unseen.iterrows():\n",
    "#     try:\n",
    "        diff_vec = []\n",
    "        a_word_vec = str_to_vec_rep(row['a_word_vec'])\n",
    "        b_word_vec = str_to_vec_rep(row['b_word_vec'])\n",
    "\n",
    "        a_sentence_vec = str_to_vec_rep(row['a_sen_vec'])\n",
    "        b_sentence_vec = str_to_vec_rep(row['b_sen_vec'])\n",
    "        \n",
    "        a_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_a'])\n",
    "        b_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_b'])\n",
    "                \n",
    "        diff_vec.append(np.concatenate([(b_word_vec - a_word_vec) ** 2, \n",
    "                                        (b_sentence_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - a_sentence_rep_vec) ** 2,\n",
    "                                        (a_sentence_rep_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - b_sentence_vec) ** 2]))\n",
    "                \n",
    "        diff_vec.append(row['edit_distance'])\n",
    "        diff_vec.append(row['capitalized_edit_distance'])\n",
    "        diff_vec.append(row['mention_b'])\n",
    "        diff_vec.append(row['mention_a'])\n",
    "        diff_vec.append(row['text_b'])\n",
    "        diff_vec.append(row['text_a'])\n",
    "        final_vecs_pos_unseen.append(diff_vec)\n",
    "        pos_labels_unseen.append(1)\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         print(b_sentence_rep_vec)\n",
    "#         print('*************', len(b_sentence_rep_vec), '**************')\n",
    "# final_vecs_pos = np.array(final_vecs_pos)\n",
    "\n",
    "for index, row in neg_unseen.iterrows():\n",
    "#     try:\n",
    "        diff_vec = []\n",
    "        a_word_vec = str_to_vec_rep(row['a_word_vec'])\n",
    "        b_word_vec = str_to_vec_rep(row['b_word_vec'])\n",
    "\n",
    "        a_sentence_vec = str_to_vec_rep(row['a_sen_vec'])\n",
    "        b_sentence_vec = str_to_vec_rep(row['b_sen_vec'])\n",
    "\n",
    "        a_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_a'])\n",
    "        b_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_b'])\n",
    "\n",
    "        diff_vec.append(np.concatenate([(b_word_vec - a_word_vec) ** 2, \n",
    "                                        (b_sentence_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - a_sentence_rep_vec) ** 2,\n",
    "                                        (a_sentence_rep_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - b_sentence_vec) ** 2]))\n",
    "        diff_vec.append(row['edit_distance'])\n",
    "        diff_vec.append(row['capitalized_edit_distance'])\n",
    "        diff_vec.append(row['mention_b'])\n",
    "        diff_vec.append(row['mention_a'])\n",
    "        diff_vec.append(row['text_b'])\n",
    "        diff_vec.append(row['text_a'])\n",
    "        final_vecs_neg_unseen.append(diff_vec)\n",
    "        neg_labels_unseen.append(0)\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         print('index_b: ', index)\n",
    "# final_vecs_neg = np.array(final_vecs_neg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vecs_pos = []\n",
    "final_vecs_neg = []\n",
    "# all_data = [pos, neg]\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "y = 0\n",
    "# pos_labels = np.array(pos['same_entity'])\n",
    "# neg_labels = np.array(neg['same_entity'])\n",
    "# max_amount = 1000\n",
    "# pos = pos.sample(frac=1)\n",
    "# neg = neg.sample(frac=1)\n",
    "\n",
    "for index, row in pos_train.iterrows():\n",
    "#     try:\n",
    "        diff_vec = []\n",
    "        a_word_vec = str_to_vec_rep(row['a_word_vec'])\n",
    "        b_word_vec = str_to_vec_rep(row['b_word_vec'])\n",
    "\n",
    "        a_sentence_vec = str_to_vec_rep(row['a_sen_vec'])\n",
    "        b_sentence_vec = str_to_vec_rep(row['b_sen_vec'])\n",
    "        \n",
    "        a_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_a'])\n",
    "        b_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_b'])\n",
    "                \n",
    "        diff_vec.append(np.concatenate([(b_word_vec - a_word_vec) ** 2, \n",
    "                                        (b_sentence_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - a_sentence_rep_vec) ** 2,\n",
    "                                        (a_sentence_rep_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - b_sentence_vec) ** 2]))\n",
    "                \n",
    "        diff_vec.append(row['edit_distance'])\n",
    "        diff_vec.append(row['capitalized_edit_distance'])\n",
    "        diff_vec.append(row['mention_b'])\n",
    "        diff_vec.append(row['mention_a'])\n",
    "        diff_vec.append(row['text_b'])\n",
    "        diff_vec.append(row['text_a'])\n",
    "        final_vecs_pos.append(diff_vec)\n",
    "        pos_labels.append(1)\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         print(b_sentence_rep_vec)\n",
    "#         print('*************', len(b_sentence_rep_vec), '**************')\n",
    "# final_vecs_pos = np.array(final_vecs_pos)\n",
    "\n",
    "for index, row in neg_train.iterrows():\n",
    "#     try:\n",
    "        diff_vec = []\n",
    "        a_word_vec = str_to_vec_rep(row['a_word_vec'])\n",
    "        b_word_vec = str_to_vec_rep(row['b_word_vec'])\n",
    "\n",
    "        a_sentence_vec = str_to_vec_rep(row['a_sen_vec'])\n",
    "        b_sentence_vec = str_to_vec_rep(row['b_sen_vec'])\n",
    "\n",
    "        a_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_a'])\n",
    "        b_sentence_rep_vec = str_to_vec_rep(row['embed_replaced_sent_b'])\n",
    "\n",
    "        diff_vec.append(np.concatenate([(b_word_vec - a_word_vec) ** 2, \n",
    "                                        (b_sentence_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - a_sentence_rep_vec) ** 2,\n",
    "                                        (a_sentence_rep_vec - a_sentence_vec) ** 2,\n",
    "                                        (b_sentence_rep_vec - b_sentence_vec) ** 2]))\n",
    "        diff_vec.append(row['edit_distance'])\n",
    "        diff_vec.append(row['capitalized_edit_distance'])\n",
    "        diff_vec.append(row['mention_b'])\n",
    "        diff_vec.append(row['mention_a'])\n",
    "        diff_vec.append(row['text_b'])\n",
    "        diff_vec.append(row['text_a'])\n",
    "        final_vecs_neg.append(diff_vec)\n",
    "        neg_labels.append(0)\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         print('index_b: ', index)\n",
    "# final_vecs_neg = np.array(final_vecs_neg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2426"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_vecs_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vecs_p = []\n",
    "final_vecs_n = []\n",
    "\n",
    "from random import shuffle\n",
    "shuffle_pos_unseen = shuffle(final_vecs_pos_unseen)\n",
    "shuffle_neg_unseen = shuffle(final_vecs_neg_unseen)\n",
    "\n",
    "\n",
    "final_vecs_p.extend(final_vecs_pos)\n",
    "final_vecs_p.extend(shuffle_pos_unseen[:1000])\n",
    "\n",
    "final_vecs_n.extend(final_vecs_neg)\n",
    "final_vecs_n.extend(shuffle_neg_unseen[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<DOC id=\"AFP_ENG_20100401.0328\" type=\"story\" >\\n<HEADLINE>\\nMaimed Lebanon journalist named international press heroine\\n</HEADLINE>\\n<DATELINE>\\nVienna, April 1, 2010 (AFP)\\n</DATELINE>\\n<TEXT>\\n<P>\\nA Lebanese journalist who was maimed in a car bomb attack five years ago vowed\\nshe would never be silenced after winning the International Press Institute\\'s\\naward for heroism on Thursday.\\n</P>\\n<P>\\nThe Vienna-based IPI said that May Chidiac\\'s bravery since losing one of her\\nhands and one of her legs when a bomb went off under her car in September 2005\\nhad been a source of inspiration for journalists around the world.\\n</P>\\n<P>\\n\"May Chidiac’s refusal as a journalist to bow to the threat of violence nearly\\ncost her her life, but has won her admiration around the world for her courage\\nand resilience,\" IPI director David Dadge said in a statement.\\n</P>\\n<P>\\nChidiac, a journalist for the private LBC network who has been openly criticial\\nof Syria\\'s influence in neighbouring Lebanon, said she herself was inspired by\\nthe work of colleagues who have lost their lives.\\n</P>\\n<P>\\n\"I will never regret speaking my mind,\" she said.\\n</P>\\n<P>\\n\"I have to be the voice of those who are not here anymore. This is now my\\nmission. Nothing will ever stop me from saying the truth and what should be\\nsaid.\"\\n</P>\\n<P>\\nChidiac returned to work in July 2006 after undergoing a series of operations in\\nParis to attach an artificial hand and leg in the wake of the attack on the\\nnorthern outskirts of the Lebanese capital Beiruit.\\n</P>\\n<P>\\nLebanon was plagued by a series of assassinations and attempted assassinations\\nbetween October 2004 and September 2005, notably the murder of former premier\\nRafiq Hariri in a massive February 14 bomb blast.\\n</P>\\n<P>\\nTwo other Lebanese journalists, Samir Kassir and Gebran Tueni, were among those\\nkilled in similar attacks.\\n</P>\\n<P>\\nOther journalists to have recently been honoured by the IPI include the\\nassassinated Russian reporter Anna Politkovskaya and Sri Lankan correspondent\\nLasantha Wickrematunge who was shot dead in January 2009.\\n</P>\\n</TEXT>\\n</DOC>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train.iloc[0]['all_text_a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193\n",
      "1391\n",
      "2244\n",
      "3584\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(final_vecs_pos)):\n",
    "     if sum(final_vecs_pos[i][0]) == 0:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "x.extend(final_vecs_pos[1193][0])\n",
    "x.append(final_vecs_pos[1193][1])\n",
    "x.append(final_vecs_pos[1193][2])\n",
    "rf.predict([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict([[0] * 2562])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pos = []\n",
    "pos_l = []\n",
    "for v in final_vecs_pos:\n",
    "    if len(v[0]) == 2560:\n",
    "        res_pos.append(v)\n",
    "        pos_l.append(1)\n",
    "\n",
    "res_neg = []\n",
    "neg_l = []\n",
    "for v in final_vecs_neg:\n",
    "    if len(v[0]) == 2560:\n",
    "        res_neg.append(v)\n",
    "        neg_l.append(0)\n",
    "        \n",
    "res_pos = np.array(res_pos)\n",
    "res_neg = np.array(res_neg)\n",
    "pos_l = np.array(pos_l)\n",
    "neg_l = np.array(neg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pos_2 = []\n",
    "pos_l_2 = []\n",
    "for v in final_vecs_pos_unseen:\n",
    "    if len(v[0]) == 2560:\n",
    "        res_pos_2.append(v)\n",
    "        pos_l_2.append(1)\n",
    "\n",
    "res_neg_2 = []\n",
    "neg_l_2 = []\n",
    "for v in final_vecs_neg_unseen:\n",
    "    if len(v[0]) == 2560:\n",
    "        res_neg_2.append(v)\n",
    "        neg_l_2.append(0)\n",
    "        \n",
    "res_pos_unseen = np.array(res_pos_2)\n",
    "res_neg_unseen = np.array(res_neg_2)\n",
    "pos_l_unseen = np.array(pos_l_2)\n",
    "neg_l_unseen = np.array(neg_l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2471"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "train features:  (449, 7)\n",
      "test features:  (4047, 7)\n",
      "train labels:  (449,)\n",
      "test labels:  (4047,)\n",
      "\n",
      "\n",
      "NEGATIVE\n",
      "train features:  (446, 7)\n",
      "test features:  (4020, 7)\n",
      "train labels:  (446,)\n",
      "test labels:  (4020,)\n",
      "\n",
      "\n",
      "TOTAL\n",
      "(895, 7)\n",
      "(8067, 7)\n",
      "(895,)\n",
      "(8067,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_pos_unseen, test_features_pos_unseen, train_labels_pos_unseen, test_labels_pos_unseen = train_test_split(res_pos_unseen, pos_l_unseen, test_size = 0.9, random_state=0)\n",
    "print('POSITIVE')\n",
    "print('train features: ', train_features_pos_unseen.shape)\n",
    "print('test features: ', test_features_pos_unseen.shape)\n",
    "print('train labels: ', train_labels_pos_unseen.shape)\n",
    "print('test labels: ', test_labels_pos_unseen.shape)\n",
    "print('\\n')\n",
    "\n",
    "train_features_neg_unseen, test_features_neg_unseen, train_labels_neg_unseen, test_labels_neg_unseen = train_test_split(res_neg_unseen, neg_l_unseen, test_size = 0.9, random_state=0)\n",
    "print('NEGATIVE')\n",
    "print('train features: ', train_features_neg_unseen.shape)\n",
    "print('test features: ', test_features_neg_unseen.shape)\n",
    "print('train labels: ', train_labels_neg_unseen.shape)\n",
    "print('test labels: ', test_labels_neg_unseen.shape)\n",
    "print('\\n')\n",
    "\n",
    "train_features_pos_unseen = list(train_features_pos_unseen)\n",
    "train_features_pos_unseen.extend(list(train_features_neg_unseen))\n",
    "train_features_list_unseen = train_features_pos_unseen\n",
    "train_features_unseen = [x[0] for x in train_features_pos_unseen]\n",
    "train_features_unseen = np.array(train_features_pos_unseen)\n",
    "\n",
    "test_features_pos_unseen = list(test_features_pos_unseen)\n",
    "test_features_pos_unseen.extend(list(test_features_neg_unseen))\n",
    "test_features_list_unseen = test_features_pos_unseen\n",
    "test_features_unseen = [x[0] for x in test_features_pos_unseen]\n",
    "test_features_unseen = np.array(test_features_pos_unseen)\n",
    "\n",
    "train_labels_pos_unseen = list(train_labels_pos_unseen)\n",
    "train_labels_pos_unseen.extend(list(train_labels_neg_unseen))\n",
    "train_labels_unseen = np.array(train_labels_pos_unseen)\n",
    "\n",
    "test_labels_pos_unseen = list(test_labels_pos_unseen)\n",
    "test_labels_pos_unseen.extend(list(test_labels_neg_unseen))\n",
    "test_labels_unseen = np.array(test_labels_pos_unseen)\n",
    "\n",
    "print('TOTAL')\n",
    "print(train_features_unseen.shape)\n",
    "print(test_features_unseen.shape)\n",
    "print(train_labels_unseen.shape)\n",
    "print(test_labels_unseen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE\n",
      "train features:  (1698, 7)\n",
      "test features:  (728, 7)\n",
      "train labels:  (1698,)\n",
      "test labels:  (728,)\n",
      "\n",
      "\n",
      "NEGATIVE\n",
      "train features:  (1729, 7)\n",
      "test features:  (742, 7)\n",
      "train labels:  (1729,)\n",
      "test labels:  (742,)\n",
      "\n",
      "\n",
      "TOTAL\n",
      "(3427, 7)\n",
      "(1470, 7)\n",
      "(3427,)\n",
      "(1470,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features_pos, test_features_pos, train_labels_pos, test_labels_pos = train_test_split(res_pos, pos_l, test_size = 0.3, random_state=0)\n",
    "print('POSITIVE')\n",
    "print('train features: ', train_features_pos.shape)\n",
    "print('test features: ', test_features_pos.shape)\n",
    "print('train labels: ', train_labels_pos.shape)\n",
    "print('test labels: ', test_labels_pos.shape)\n",
    "print('\\n')\n",
    "\n",
    "train_features_neg, test_features_neg, train_labels_neg, test_labels_neg = train_test_split(res_neg, neg_l, test_size = 0.3, random_state=0)\n",
    "print('NEGATIVE')\n",
    "print('train features: ', train_features_neg.shape)\n",
    "print('test features: ', test_features_neg.shape)\n",
    "print('train labels: ', train_labels_neg.shape)\n",
    "print('test labels: ', test_labels_neg.shape)\n",
    "print('\\n')\n",
    "\n",
    "train_features_pos = list(train_features_pos)\n",
    "train_features_pos.extend(list(train_features_neg))\n",
    "train_features_list = train_features_pos\n",
    "train_features = [x[0] for x in train_features_pos]\n",
    "train_features = np.array(train_features_pos)\n",
    "\n",
    "test_features_pos = list(test_features_pos)\n",
    "test_features_pos.extend(list(test_features_neg))\n",
    "test_features_list = test_features_pos\n",
    "test_features = [x[0] for x in test_features_pos]\n",
    "test_features = np.array(test_features_pos)\n",
    "\n",
    "train_labels_pos = list(train_labels_pos)\n",
    "train_labels_pos.extend(list(train_labels_neg))\n",
    "train_labels = np.array(train_labels_pos)\n",
    "\n",
    "test_labels_pos = list(test_labels_pos)\n",
    "test_labels_pos.extend(list(test_labels_neg))\n",
    "test_labels = np.array(test_labels_pos)\n",
    "\n",
    "print('TOTAL')\n",
    "print(train_features.shape)\n",
    "print(test_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_tmp = []\n",
    "train_labels_tmp = []\n",
    "for x, y in zip(train_features, train_labels):\n",
    "    if x.shape[0] == 192 and np.all(np.isfinite(x)):\n",
    "        train_features_tmp.append(x)\n",
    "        train_labels_tmp.append(y)\n",
    "        \n",
    "train_features = np.array(train_features_tmp)\n",
    "train_labels = np.array(train_labels_tmp)\n",
    "\n",
    "test_features_tmp = []\n",
    "test_labels_tmp = []\n",
    "for x, y in zip(test_features, test_labels):        \n",
    "    if x.shape[0] == 192 and np.all(np.isfinite(x)):\n",
    "        test_features_tmp.append(x)\n",
    "        test_labels_tmp.append(y)\n",
    "        \n",
    "test_features = np.array(test_features_tmp)\n",
    "test_labels = np.array(test_labels_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('test_fatures_zero_shot_unseen.pickle', 'wb') as handle:\n",
    "# with open('yago_test_features.pickle', 'wb') as handle:\n",
    "with open('tac_test_features.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_features, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pairs_classifier_zero_shot.pickle', 'rb') as handle:\n",
    "    rf = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_combined = []\n",
    "train_features_combined.extend(train_features)\n",
    "train_features_combined.extend(train_features_unseen)\n",
    "\n",
    "train_labels_combined = []\n",
    "train_labels_combined.extend(train_labels)\n",
    "train_labels_combined.extend(train_labels_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_combined = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0., 0., 0., ..., 0., 0., 0.]), 0, 2, 'Kuper', 'Kuper',\n",
       "       'NYT_ENG_20100303.0149', 'APW_ENG_20101029.0706'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6077207\ttotal: 396ms\tremaining: 1m 18s\n",
      "1:\tlearn: 0.5368995\ttotal: 579ms\tremaining: 57.3s\n",
      "2:\tlearn: 0.4711965\ttotal: 762ms\tremaining: 50s\n",
      "3:\tlearn: 0.4135713\ttotal: 944ms\tremaining: 46.3s\n",
      "4:\tlearn: 0.3621853\ttotal: 1.13s\tremaining: 44.2s\n",
      "5:\tlearn: 0.3241407\ttotal: 1.32s\tremaining: 42.7s\n",
      "6:\tlearn: 0.2868075\ttotal: 1.51s\tremaining: 41.6s\n",
      "7:\tlearn: 0.2544745\ttotal: 1.69s\tremaining: 40.6s\n",
      "8:\tlearn: 0.2268514\ttotal: 1.89s\tremaining: 40.1s\n",
      "9:\tlearn: 0.2002095\ttotal: 2.1s\tremaining: 39.9s\n",
      "10:\tlearn: 0.1778174\ttotal: 2.31s\tremaining: 39.7s\n",
      "11:\tlearn: 0.1592624\ttotal: 2.5s\tremaining: 39.1s\n",
      "12:\tlearn: 0.1409125\ttotal: 2.69s\tremaining: 38.7s\n",
      "13:\tlearn: 0.1273701\ttotal: 2.88s\tremaining: 38.2s\n",
      "14:\tlearn: 0.1144225\ttotal: 3.07s\tremaining: 37.9s\n",
      "15:\tlearn: 0.1026348\ttotal: 3.25s\tremaining: 37.4s\n",
      "16:\tlearn: 0.0914779\ttotal: 3.45s\tremaining: 37.1s\n",
      "17:\tlearn: 0.0827080\ttotal: 3.65s\tremaining: 36.9s\n",
      "18:\tlearn: 0.0745150\ttotal: 3.85s\tremaining: 36.7s\n",
      "19:\tlearn: 0.0675357\ttotal: 4.03s\tremaining: 36.3s\n",
      "20:\tlearn: 0.0610829\ttotal: 4.27s\tremaining: 36.4s\n",
      "21:\tlearn: 0.0554557\ttotal: 4.46s\tremaining: 36.1s\n",
      "22:\tlearn: 0.0503657\ttotal: 4.65s\tremaining: 35.8s\n",
      "23:\tlearn: 0.0459208\ttotal: 4.84s\tremaining: 35.5s\n",
      "24:\tlearn: 0.0410842\ttotal: 5.03s\tremaining: 35.2s\n",
      "25:\tlearn: 0.0378968\ttotal: 5.23s\tremaining: 35s\n",
      "26:\tlearn: 0.0347424\ttotal: 5.43s\tremaining: 34.8s\n",
      "27:\tlearn: 0.0318395\ttotal: 5.63s\tremaining: 34.6s\n",
      "28:\tlearn: 0.0294802\ttotal: 5.83s\tremaining: 34.3s\n",
      "29:\tlearn: 0.0271131\ttotal: 6.03s\tremaining: 34.2s\n",
      "30:\tlearn: 0.0250645\ttotal: 6.24s\tremaining: 34s\n",
      "31:\tlearn: 0.0232797\ttotal: 6.44s\tremaining: 33.8s\n",
      "32:\tlearn: 0.0217877\ttotal: 6.64s\tremaining: 33.6s\n",
      "33:\tlearn: 0.0204341\ttotal: 6.83s\tremaining: 33.4s\n",
      "34:\tlearn: 0.0190425\ttotal: 7.03s\tremaining: 33.2s\n",
      "35:\tlearn: 0.0178596\ttotal: 7.24s\tremaining: 33s\n",
      "36:\tlearn: 0.0163736\ttotal: 7.47s\tremaining: 32.9s\n",
      "37:\tlearn: 0.0154118\ttotal: 7.67s\tremaining: 32.7s\n",
      "38:\tlearn: 0.0145655\ttotal: 7.89s\tremaining: 32.6s\n",
      "39:\tlearn: 0.0135408\ttotal: 8.11s\tremaining: 32.4s\n",
      "40:\tlearn: 0.0125731\ttotal: 8.31s\tremaining: 32.2s\n",
      "41:\tlearn: 0.0118543\ttotal: 8.54s\tremaining: 32.1s\n",
      "42:\tlearn: 0.0113476\ttotal: 8.73s\tremaining: 31.9s\n",
      "43:\tlearn: 0.0108520\ttotal: 8.94s\tremaining: 31.7s\n",
      "44:\tlearn: 0.0103884\ttotal: 9.14s\tremaining: 31.5s\n",
      "45:\tlearn: 0.0097967\ttotal: 9.35s\tremaining: 31.3s\n",
      "46:\tlearn: 0.0093105\ttotal: 9.55s\tremaining: 31.1s\n",
      "47:\tlearn: 0.0088639\ttotal: 9.76s\tremaining: 30.9s\n",
      "48:\tlearn: 0.0083616\ttotal: 9.96s\tremaining: 30.7s\n",
      "49:\tlearn: 0.0078731\ttotal: 10.2s\tremaining: 30.5s\n",
      "50:\tlearn: 0.0073242\ttotal: 10.4s\tremaining: 30.3s\n",
      "51:\tlearn: 0.0070077\ttotal: 10.6s\tremaining: 30.1s\n",
      "52:\tlearn: 0.0066533\ttotal: 10.8s\tremaining: 29.9s\n",
      "53:\tlearn: 0.0064239\ttotal: 11s\tremaining: 29.7s\n",
      "54:\tlearn: 0.0060988\ttotal: 11.2s\tremaining: 29.5s\n",
      "55:\tlearn: 0.0059113\ttotal: 11.4s\tremaining: 29.3s\n",
      "56:\tlearn: 0.0056378\ttotal: 11.6s\tremaining: 29.1s\n",
      "57:\tlearn: 0.0054372\ttotal: 11.8s\tremaining: 28.9s\n",
      "58:\tlearn: 0.0051988\ttotal: 12s\tremaining: 28.7s\n",
      "59:\tlearn: 0.0050104\ttotal: 12.2s\tremaining: 28.5s\n",
      "60:\tlearn: 0.0048165\ttotal: 12.4s\tremaining: 28.3s\n",
      "61:\tlearn: 0.0046206\ttotal: 12.6s\tremaining: 28.1s\n",
      "62:\tlearn: 0.0044986\ttotal: 12.9s\tremaining: 28s\n",
      "63:\tlearn: 0.0043100\ttotal: 13.1s\tremaining: 27.8s\n",
      "64:\tlearn: 0.0041273\ttotal: 13.3s\tremaining: 27.6s\n",
      "65:\tlearn: 0.0040232\ttotal: 13.5s\tremaining: 27.4s\n",
      "66:\tlearn: 0.0038785\ttotal: 13.7s\tremaining: 27.2s\n",
      "67:\tlearn: 0.0037525\ttotal: 13.9s\tremaining: 27s\n",
      "68:\tlearn: 0.0036424\ttotal: 14.1s\tremaining: 26.8s\n",
      "69:\tlearn: 0.0035088\ttotal: 14.4s\tremaining: 26.7s\n",
      "70:\tlearn: 0.0033902\ttotal: 14.6s\tremaining: 26.4s\n",
      "71:\tlearn: 0.0032586\ttotal: 14.8s\tremaining: 26.3s\n",
      "72:\tlearn: 0.0031547\ttotal: 15s\tremaining: 26.1s\n",
      "73:\tlearn: 0.0030806\ttotal: 15.2s\tremaining: 25.9s\n",
      "74:\tlearn: 0.0029744\ttotal: 15.4s\tremaining: 25.7s\n",
      "75:\tlearn: 0.0028833\ttotal: 15.6s\tremaining: 25.5s\n",
      "76:\tlearn: 0.0028121\ttotal: 15.9s\tremaining: 25.3s\n",
      "77:\tlearn: 0.0027283\ttotal: 16.1s\tremaining: 25.1s\n",
      "78:\tlearn: 0.0026434\ttotal: 16.3s\tremaining: 25s\n",
      "79:\tlearn: 0.0025598\ttotal: 16.6s\tremaining: 24.8s\n",
      "80:\tlearn: 0.0024907\ttotal: 16.8s\tremaining: 24.6s\n",
      "81:\tlearn: 0.0023879\ttotal: 17s\tremaining: 24.4s\n",
      "82:\tlearn: 0.0023429\ttotal: 17.2s\tremaining: 24.3s\n",
      "83:\tlearn: 0.0022804\ttotal: 17.4s\tremaining: 24.1s\n",
      "84:\tlearn: 0.0022281\ttotal: 17.7s\tremaining: 23.9s\n",
      "85:\tlearn: 0.0021714\ttotal: 17.9s\tremaining: 23.7s\n",
      "86:\tlearn: 0.0021099\ttotal: 18.1s\tremaining: 23.6s\n",
      "87:\tlearn: 0.0020641\ttotal: 18.4s\tremaining: 23.4s\n",
      "88:\tlearn: 0.0020137\ttotal: 18.6s\tremaining: 23.2s\n",
      "89:\tlearn: 0.0019539\ttotal: 18.8s\tremaining: 23s\n",
      "90:\tlearn: 0.0019127\ttotal: 19s\tremaining: 22.8s\n",
      "91:\tlearn: 0.0018782\ttotal: 19.3s\tremaining: 22.6s\n",
      "92:\tlearn: 0.0018431\ttotal: 19.5s\tremaining: 22.4s\n",
      "93:\tlearn: 0.0018012\ttotal: 19.7s\tremaining: 22.2s\n",
      "94:\tlearn: 0.0017571\ttotal: 19.9s\tremaining: 22s\n",
      "95:\tlearn: 0.0017064\ttotal: 20.2s\tremaining: 21.8s\n",
      "96:\tlearn: 0.0016604\ttotal: 20.4s\tremaining: 21.6s\n",
      "97:\tlearn: 0.0016256\ttotal: 20.6s\tremaining: 21.4s\n",
      "98:\tlearn: 0.0016020\ttotal: 20.8s\tremaining: 21.2s\n",
      "99:\tlearn: 0.0015599\ttotal: 21s\tremaining: 21s\n",
      "100:\tlearn: 0.0015319\ttotal: 21.3s\tremaining: 20.8s\n",
      "101:\tlearn: 0.0015042\ttotal: 21.5s\tremaining: 20.7s\n",
      "102:\tlearn: 0.0014640\ttotal: 21.7s\tremaining: 20.5s\n",
      "103:\tlearn: 0.0014360\ttotal: 22s\tremaining: 20.3s\n",
      "104:\tlearn: 0.0014089\ttotal: 22.2s\tremaining: 20.1s\n",
      "105:\tlearn: 0.0013837\ttotal: 22.4s\tremaining: 19.9s\n",
      "106:\tlearn: 0.0013528\ttotal: 22.6s\tremaining: 19.7s\n",
      "107:\tlearn: 0.0013295\ttotal: 22.8s\tremaining: 19.5s\n",
      "108:\tlearn: 0.0013072\ttotal: 23.1s\tremaining: 19.3s\n",
      "109:\tlearn: 0.0012846\ttotal: 23.3s\tremaining: 19.1s\n",
      "110:\tlearn: 0.0012584\ttotal: 23.5s\tremaining: 18.9s\n",
      "111:\tlearn: 0.0012391\ttotal: 23.8s\tremaining: 18.7s\n",
      "112:\tlearn: 0.0012165\ttotal: 24s\tremaining: 18.5s\n",
      "113:\tlearn: 0.0011975\ttotal: 24.3s\tremaining: 18.3s\n",
      "114:\tlearn: 0.0011780\ttotal: 24.5s\tremaining: 18.1s\n",
      "115:\tlearn: 0.0011621\ttotal: 24.7s\tremaining: 17.9s\n",
      "116:\tlearn: 0.0011401\ttotal: 24.9s\tremaining: 17.7s\n",
      "117:\tlearn: 0.0011173\ttotal: 25.2s\tremaining: 17.5s\n",
      "118:\tlearn: 0.0010935\ttotal: 25.4s\tremaining: 17.3s\n",
      "119:\tlearn: 0.0010720\ttotal: 25.6s\tremaining: 17.1s\n",
      "120:\tlearn: 0.0010526\ttotal: 25.8s\tremaining: 16.9s\n",
      "121:\tlearn: 0.0010373\ttotal: 26.1s\tremaining: 16.7s\n",
      "122:\tlearn: 0.0010207\ttotal: 26.3s\tremaining: 16.5s\n",
      "123:\tlearn: 0.0010068\ttotal: 26.5s\tremaining: 16.3s\n",
      "124:\tlearn: 0.0009896\ttotal: 26.7s\tremaining: 16s\n",
      "125:\tlearn: 0.0009753\ttotal: 27s\tremaining: 15.8s\n",
      "126:\tlearn: 0.0009600\ttotal: 27.2s\tremaining: 15.6s\n",
      "127:\tlearn: 0.0009489\ttotal: 27.4s\tremaining: 15.4s\n",
      "128:\tlearn: 0.0009345\ttotal: 27.7s\tremaining: 15.2s\n",
      "129:\tlearn: 0.0009206\ttotal: 27.9s\tremaining: 15s\n",
      "130:\tlearn: 0.0009036\ttotal: 28.2s\tremaining: 14.9s\n",
      "131:\tlearn: 0.0008933\ttotal: 28.5s\tremaining: 14.7s\n",
      "132:\tlearn: 0.0008841\ttotal: 28.7s\tremaining: 14.4s\n",
      "133:\tlearn: 0.0008736\ttotal: 28.9s\tremaining: 14.2s\n",
      "134:\tlearn: 0.0008617\ttotal: 29.1s\tremaining: 14s\n",
      "135:\tlearn: 0.0008521\ttotal: 29.4s\tremaining: 13.8s\n",
      "136:\tlearn: 0.0008398\ttotal: 29.6s\tremaining: 13.6s\n",
      "137:\tlearn: 0.0008306\ttotal: 29.9s\tremaining: 13.4s\n",
      "138:\tlearn: 0.0008192\ttotal: 30.1s\tremaining: 13.2s\n",
      "139:\tlearn: 0.0008092\ttotal: 30.3s\tremaining: 13s\n",
      "140:\tlearn: 0.0007989\ttotal: 30.5s\tremaining: 12.8s\n",
      "141:\tlearn: 0.0007890\ttotal: 30.8s\tremaining: 12.6s\n",
      "142:\tlearn: 0.0007890\ttotal: 31s\tremaining: 12.4s\n",
      "143:\tlearn: 0.0007890\ttotal: 31.2s\tremaining: 12.1s\n",
      "144:\tlearn: 0.0007801\ttotal: 31.5s\tremaining: 11.9s\n",
      "145:\tlearn: 0.0007686\ttotal: 31.7s\tremaining: 11.7s\n",
      "146:\tlearn: 0.0007601\ttotal: 31.9s\tremaining: 11.5s\n",
      "147:\tlearn: 0.0007489\ttotal: 32.2s\tremaining: 11.3s\n",
      "148:\tlearn: 0.0007380\ttotal: 32.4s\tremaining: 11.1s\n",
      "149:\tlearn: 0.0007278\ttotal: 32.6s\tremaining: 10.9s\n",
      "150:\tlearn: 0.0007278\ttotal: 32.9s\tremaining: 10.7s\n",
      "151:\tlearn: 0.0007278\ttotal: 33.1s\tremaining: 10.5s\n",
      "152:\tlearn: 0.0007278\ttotal: 33.4s\tremaining: 10.3s\n",
      "153:\tlearn: 0.0007278\ttotal: 33.7s\tremaining: 10.1s\n",
      "154:\tlearn: 0.0007278\ttotal: 33.9s\tremaining: 9.85s\n",
      "155:\tlearn: 0.0007278\ttotal: 34.2s\tremaining: 9.63s\n",
      "156:\tlearn: 0.0007278\ttotal: 34.4s\tremaining: 9.43s\n",
      "157:\tlearn: 0.0007192\ttotal: 34.7s\tremaining: 9.23s\n",
      "158:\tlearn: 0.0007192\ttotal: 34.9s\tremaining: 9.01s\n",
      "159:\tlearn: 0.0007192\ttotal: 35.2s\tremaining: 8.79s\n",
      "160:\tlearn: 0.0007192\ttotal: 35.4s\tremaining: 8.57s\n",
      "161:\tlearn: 0.0007192\ttotal: 35.6s\tremaining: 8.36s\n",
      "162:\tlearn: 0.0007191\ttotal: 35.9s\tremaining: 8.14s\n",
      "163:\tlearn: 0.0007191\ttotal: 36.1s\tremaining: 7.93s\n",
      "164:\tlearn: 0.0007093\ttotal: 36.3s\tremaining: 7.71s\n",
      "165:\tlearn: 0.0007093\ttotal: 36.6s\tremaining: 7.49s\n",
      "166:\tlearn: 0.0007093\ttotal: 36.8s\tremaining: 7.27s\n",
      "167:\tlearn: 0.0007093\ttotal: 37s\tremaining: 7.06s\n",
      "168:\tlearn: 0.0007093\ttotal: 37.3s\tremaining: 6.83s\n",
      "169:\tlearn: 0.0007002\ttotal: 37.5s\tremaining: 6.62s\n",
      "170:\tlearn: 0.0007002\ttotal: 37.7s\tremaining: 6.4s\n",
      "171:\tlearn: 0.0007002\ttotal: 38s\tremaining: 6.18s\n",
      "172:\tlearn: 0.0007002\ttotal: 38.2s\tremaining: 5.96s\n",
      "173:\tlearn: 0.0007002\ttotal: 38.5s\tremaining: 5.76s\n",
      "174:\tlearn: 0.0006906\ttotal: 38.8s\tremaining: 5.54s\n",
      "175:\tlearn: 0.0006906\ttotal: 39s\tremaining: 5.32s\n",
      "176:\tlearn: 0.0006906\ttotal: 39.2s\tremaining: 5.1s\n",
      "177:\tlearn: 0.0006823\ttotal: 39.5s\tremaining: 4.88s\n",
      "178:\tlearn: 0.0006823\ttotal: 39.7s\tremaining: 4.66s\n",
      "179:\tlearn: 0.0006823\ttotal: 39.9s\tremaining: 4.44s\n",
      "180:\tlearn: 0.0006823\ttotal: 40.2s\tremaining: 4.22s\n",
      "181:\tlearn: 0.0006823\ttotal: 40.4s\tremaining: 4s\n",
      "182:\tlearn: 0.0006823\ttotal: 40.6s\tremaining: 3.78s\n",
      "183:\tlearn: 0.0006823\ttotal: 40.9s\tremaining: 3.55s\n",
      "184:\tlearn: 0.0006823\ttotal: 41.1s\tremaining: 3.33s\n",
      "185:\tlearn: 0.0006823\ttotal: 41.4s\tremaining: 3.12s\n",
      "186:\tlearn: 0.0006823\ttotal: 41.6s\tremaining: 2.89s\n",
      "187:\tlearn: 0.0006822\ttotal: 41.9s\tremaining: 2.67s\n",
      "188:\tlearn: 0.0006822\ttotal: 42.1s\tremaining: 2.45s\n",
      "189:\tlearn: 0.0006822\ttotal: 42.3s\tremaining: 2.23s\n",
      "190:\tlearn: 0.0006822\ttotal: 42.6s\tremaining: 2s\n",
      "191:\tlearn: 0.0006822\ttotal: 42.8s\tremaining: 1.78s\n",
      "192:\tlearn: 0.0006720\ttotal: 43s\tremaining: 1.56s\n",
      "193:\tlearn: 0.0006720\ttotal: 43.2s\tremaining: 1.34s\n",
      "194:\tlearn: 0.0006720\ttotal: 43.5s\tremaining: 1.11s\n",
      "195:\tlearn: 0.0006622\ttotal: 43.7s\tremaining: 892ms\n",
      "196:\tlearn: 0.0006622\ttotal: 43.9s\tremaining: 669ms\n",
      "197:\tlearn: 0.0006622\ttotal: 44.2s\tremaining: 446ms\n",
      "198:\tlearn: 0.0006622\ttotal: 44.4s\tremaining: 223ms\n",
      "199:\tlearn: 0.0006538\ttotal: 44.7s\tremaining: 0us\n",
      "roc auc:  0.5\n",
      "error:  0.4983265154332466\n",
      "f1:  0.6681525507677067\n",
      "precision : 0.5016734845667534\n",
      "recall : 1.0\n",
      "***********************\n",
      " predicted False  predicted True              \n",
      "               0            4020  actual False\n",
      "               0            4047   actual True\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "train_set = [np.append(np.append(t[0], t[1]), t[2]) for t in train_features]\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.03,\n",
    "    random_seed=63\n",
    ")\n",
    "model.fit(\n",
    "    train_set, train_labels,\n",
    "#     cat_features=cat_features,\n",
    "#     eval_set=(X_validation, y_validation),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.predict([np.append(np.append(t[0], t[1]), t[2]) for t in test_features_unseen])\n",
    "errors = abs(predictions - test_labels_unseen)\n",
    "roc_auc = roc_auc_score(test_labels_unseen, predictions)\n",
    "print('roc auc: ', roc_auc)\n",
    "print('error: ', sum(errors)/len(predictions))\n",
    "print('f1: ', f1_score(test_labels_unseen, predictions))\n",
    "print('precision :', precision_score(test_labels_unseen, predictions))\n",
    "print('recall :', recall_score(test_labels_unseen, predictions))  \n",
    "print('***********************')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(test_labels_unseen, predictions))\n",
    "confusion_df.columns = ['predicted False', 'predicted True']\n",
    "confusion_df[''] = ['actual False', 'actual True']\n",
    "print(confusion_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "895"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 3.5013008\ttotal: 501ms\tremaining: 1m 39s\n",
      "1:\tlearn: 3.4016728\ttotal: 809ms\tremaining: 1m 20s\n",
      "2:\tlearn: 3.2975861\ttotal: 1.13s\tremaining: 1m 14s\n",
      "3:\tlearn: 3.2191727\ttotal: 1.44s\tremaining: 1m 10s\n",
      "4:\tlearn: 3.1198007\ttotal: 1.75s\tremaining: 1m 8s\n",
      "5:\tlearn: 3.0591464\ttotal: 2.07s\tremaining: 1m 6s\n",
      "6:\tlearn: 2.9670296\ttotal: 2.38s\tremaining: 1m 5s\n",
      "7:\tlearn: 2.8770973\ttotal: 2.7s\tremaining: 1m 4s\n",
      "8:\tlearn: 2.8102019\ttotal: 3.02s\tremaining: 1m 4s\n",
      "9:\tlearn: 2.7512535\ttotal: 3.34s\tremaining: 1m 3s\n",
      "10:\tlearn: 2.6678355\ttotal: 3.66s\tremaining: 1m 2s\n",
      "11:\tlearn: 2.5832852\ttotal: 4.03s\tremaining: 1m 3s\n",
      "12:\tlearn: 2.5046250\ttotal: 4.35s\tremaining: 1m 2s\n",
      "13:\tlearn: 2.4270472\ttotal: 4.68s\tremaining: 1m 2s\n",
      "14:\tlearn: 2.3515919\ttotal: 5s\tremaining: 1m 1s\n",
      "15:\tlearn: 2.2770974\ttotal: 5.42s\tremaining: 1m 2s\n",
      "16:\tlearn: 2.2033607\ttotal: 5.74s\tremaining: 1m 1s\n",
      "17:\tlearn: 2.1293658\ttotal: 6.07s\tremaining: 1m 1s\n",
      "18:\tlearn: 2.0683238\ttotal: 6.38s\tremaining: 1m\n",
      "19:\tlearn: 2.0031588\ttotal: 6.7s\tremaining: 1m\n",
      "20:\tlearn: 1.9443708\ttotal: 6.99s\tremaining: 59.6s\n",
      "21:\tlearn: 1.8804174\ttotal: 7.24s\tremaining: 58.6s\n",
      "22:\tlearn: 1.8204924\ttotal: 7.48s\tremaining: 57.5s\n",
      "23:\tlearn: 1.7635581\ttotal: 7.72s\tremaining: 56.7s\n",
      "24:\tlearn: 1.7090812\ttotal: 7.97s\tremaining: 55.8s\n",
      "25:\tlearn: 1.6553999\ttotal: 8.21s\tremaining: 55s\n",
      "26:\tlearn: 1.6019666\ttotal: 8.49s\tremaining: 54.4s\n",
      "27:\tlearn: 1.5523398\ttotal: 8.76s\tremaining: 53.8s\n",
      "28:\tlearn: 1.5039094\ttotal: 9.03s\tremaining: 53.3s\n",
      "29:\tlearn: 1.4559520\ttotal: 9.28s\tremaining: 52.6s\n",
      "30:\tlearn: 1.4107648\ttotal: 9.57s\tremaining: 52.2s\n",
      "31:\tlearn: 1.3682535\ttotal: 9.87s\tremaining: 51.8s\n",
      "32:\tlearn: 1.3274641\ttotal: 10.2s\tremaining: 51.5s\n",
      "33:\tlearn: 1.2863556\ttotal: 10.5s\tremaining: 51.1s\n",
      "34:\tlearn: 1.2466210\ttotal: 10.8s\tremaining: 50.8s\n",
      "35:\tlearn: 1.2098667\ttotal: 11s\tremaining: 50.3s\n",
      "36:\tlearn: 1.1724069\ttotal: 11.3s\tremaining: 49.7s\n",
      "37:\tlearn: 1.1381295\ttotal: 11.5s\tremaining: 49.1s\n",
      "38:\tlearn: 1.1041302\ttotal: 11.8s\tremaining: 48.6s\n",
      "39:\tlearn: 1.0714340\ttotal: 12s\tremaining: 48s\n",
      "40:\tlearn: 1.0413034\ttotal: 12.3s\tremaining: 47.7s\n",
      "41:\tlearn: 1.0101598\ttotal: 12.5s\tremaining: 47.1s\n",
      "42:\tlearn: 0.9819176\ttotal: 12.8s\tremaining: 46.6s\n",
      "43:\tlearn: 0.9557613\ttotal: 13.1s\tremaining: 46.3s\n",
      "44:\tlearn: 0.9306628\ttotal: 13.3s\tremaining: 45.8s\n",
      "45:\tlearn: 0.9035452\ttotal: 13.5s\tremaining: 45.3s\n",
      "46:\tlearn: 0.8808504\ttotal: 13.8s\tremaining: 44.9s\n",
      "47:\tlearn: 0.8590156\ttotal: 14s\tremaining: 44.5s\n",
      "48:\tlearn: 0.8383168\ttotal: 14.3s\tremaining: 44s\n",
      "49:\tlearn: 0.8182152\ttotal: 14.5s\tremaining: 43.6s\n",
      "50:\tlearn: 0.7990069\ttotal: 14.8s\tremaining: 43.3s\n",
      "51:\tlearn: 0.7809216\ttotal: 15.1s\tremaining: 42.9s\n",
      "52:\tlearn: 0.7639153\ttotal: 15.3s\tremaining: 42.5s\n",
      "53:\tlearn: 0.7477825\ttotal: 15.6s\tremaining: 42.1s\n",
      "54:\tlearn: 0.7305620\ttotal: 15.9s\tremaining: 42s\n",
      "55:\tlearn: 0.7154132\ttotal: 16.2s\tremaining: 41.7s\n",
      "56:\tlearn: 0.7010523\ttotal: 16.5s\tremaining: 41.3s\n",
      "57:\tlearn: 0.6873256\ttotal: 16.7s\tremaining: 40.9s\n",
      "58:\tlearn: 0.6748522\ttotal: 16.9s\tremaining: 40.5s\n",
      "59:\tlearn: 0.6621216\ttotal: 17.2s\tremaining: 40.2s\n",
      "60:\tlearn: 0.6501266\ttotal: 17.5s\tremaining: 39.8s\n",
      "61:\tlearn: 0.6391794\ttotal: 17.7s\tremaining: 39.4s\n",
      "62:\tlearn: 0.6289396\ttotal: 18s\tremaining: 39.1s\n",
      "63:\tlearn: 0.6197685\ttotal: 18.2s\tremaining: 38.8s\n",
      "64:\tlearn: 0.6104723\ttotal: 18.5s\tremaining: 38.4s\n",
      "65:\tlearn: 0.6015268\ttotal: 18.8s\tremaining: 38.1s\n",
      "66:\tlearn: 0.5934185\ttotal: 19.1s\tremaining: 37.9s\n",
      "67:\tlearn: 0.5861757\ttotal: 19.3s\tremaining: 37.5s\n",
      "68:\tlearn: 0.5784196\ttotal: 19.6s\tremaining: 37.2s\n",
      "69:\tlearn: 0.5708972\ttotal: 19.9s\tremaining: 36.9s\n",
      "70:\tlearn: 0.5645138\ttotal: 20.1s\tremaining: 36.5s\n",
      "71:\tlearn: 0.5566741\ttotal: 20.4s\tremaining: 36.2s\n",
      "72:\tlearn: 0.5502956\ttotal: 20.7s\tremaining: 36s\n",
      "73:\tlearn: 0.5448417\ttotal: 21s\tremaining: 35.7s\n",
      "74:\tlearn: 0.5391585\ttotal: 21.2s\tremaining: 35.4s\n",
      "75:\tlearn: 0.5338013\ttotal: 21.5s\tremaining: 35.1s\n",
      "76:\tlearn: 0.5290912\ttotal: 21.7s\tremaining: 34.7s\n",
      "77:\tlearn: 0.5234074\ttotal: 22s\tremaining: 34.4s\n",
      "78:\tlearn: 0.5183412\ttotal: 22.3s\tremaining: 34.1s\n",
      "79:\tlearn: 0.5129940\ttotal: 22.5s\tremaining: 33.8s\n",
      "80:\tlearn: 0.5083607\ttotal: 22.8s\tremaining: 33.5s\n",
      "81:\tlearn: 0.5035021\ttotal: 23.1s\tremaining: 33.2s\n",
      "82:\tlearn: 0.4995128\ttotal: 23.3s\tremaining: 32.9s\n",
      "83:\tlearn: 0.4951037\ttotal: 23.6s\tremaining: 32.5s\n",
      "84:\tlearn: 0.4909563\ttotal: 23.8s\tremaining: 32.2s\n",
      "85:\tlearn: 0.4879109\ttotal: 24s\tremaining: 31.9s\n",
      "86:\tlearn: 0.4842867\ttotal: 24.3s\tremaining: 31.6s\n",
      "87:\tlearn: 0.4801096\ttotal: 24.5s\tremaining: 31.2s\n",
      "88:\tlearn: 0.4768816\ttotal: 24.8s\tremaining: 30.9s\n",
      "89:\tlearn: 0.4731820\ttotal: 25.1s\tremaining: 30.6s\n",
      "90:\tlearn: 0.4698660\ttotal: 25.3s\tremaining: 30.3s\n",
      "91:\tlearn: 0.4663043\ttotal: 25.6s\tremaining: 30s\n",
      "92:\tlearn: 0.4633613\ttotal: 25.8s\tremaining: 29.7s\n",
      "93:\tlearn: 0.4602002\ttotal: 26.1s\tremaining: 29.4s\n",
      "94:\tlearn: 0.4549559\ttotal: 26.3s\tremaining: 29.1s\n",
      "95:\tlearn: 0.4522260\ttotal: 26.6s\tremaining: 28.8s\n",
      "96:\tlearn: 0.4489950\ttotal: 27s\tremaining: 28.6s\n",
      "97:\tlearn: 0.4461214\ttotal: 27.3s\tremaining: 28.4s\n",
      "98:\tlearn: 0.4436888\ttotal: 27.5s\tremaining: 28.1s\n",
      "99:\tlearn: 0.4408154\ttotal: 27.7s\tremaining: 27.7s\n",
      "100:\tlearn: 0.4382937\ttotal: 28s\tremaining: 27.4s\n",
      "101:\tlearn: 0.4355249\ttotal: 28.2s\tremaining: 27.1s\n",
      "102:\tlearn: 0.4323507\ttotal: 28.5s\tremaining: 26.9s\n",
      "103:\tlearn: 0.4298883\ttotal: 28.8s\tremaining: 26.6s\n",
      "104:\tlearn: 0.4269582\ttotal: 29s\tremaining: 26.3s\n",
      "105:\tlearn: 0.4254487\ttotal: 29.3s\tremaining: 26s\n",
      "106:\tlearn: 0.4223584\ttotal: 29.7s\tremaining: 25.8s\n",
      "107:\tlearn: 0.4193189\ttotal: 29.9s\tremaining: 25.5s\n",
      "108:\tlearn: 0.4170029\ttotal: 30.2s\tremaining: 25.2s\n",
      "109:\tlearn: 0.4142581\ttotal: 30.4s\tremaining: 24.9s\n",
      "110:\tlearn: 0.4122142\ttotal: 30.7s\tremaining: 24.6s\n",
      "111:\tlearn: 0.4085289\ttotal: 31.1s\tremaining: 24.4s\n",
      "112:\tlearn: 0.4064105\ttotal: 31.3s\tremaining: 24.1s\n",
      "113:\tlearn: 0.4045027\ttotal: 31.6s\tremaining: 23.8s\n",
      "114:\tlearn: 0.4028592\ttotal: 31.8s\tremaining: 23.5s\n",
      "115:\tlearn: 0.4009953\ttotal: 32.1s\tremaining: 23.2s\n",
      "116:\tlearn: 0.3970736\ttotal: 32.3s\tremaining: 22.9s\n",
      "117:\tlearn: 0.3947168\ttotal: 32.6s\tremaining: 22.6s\n",
      "118:\tlearn: 0.3927093\ttotal: 32.8s\tremaining: 22.3s\n",
      "119:\tlearn: 0.3905582\ttotal: 33.1s\tremaining: 22.1s\n",
      "120:\tlearn: 0.3882531\ttotal: 33.4s\tremaining: 21.8s\n",
      "121:\tlearn: 0.3864021\ttotal: 33.6s\tremaining: 21.5s\n",
      "122:\tlearn: 0.3847930\ttotal: 33.9s\tremaining: 21.2s\n",
      "123:\tlearn: 0.3824475\ttotal: 34.1s\tremaining: 20.9s\n",
      "124:\tlearn: 0.3797337\ttotal: 34.4s\tremaining: 20.6s\n",
      "125:\tlearn: 0.3777672\ttotal: 34.6s\tremaining: 20.3s\n",
      "126:\tlearn: 0.3754162\ttotal: 34.9s\tremaining: 20.1s\n",
      "127:\tlearn: 0.3735772\ttotal: 35.2s\tremaining: 19.8s\n",
      "128:\tlearn: 0.3719182\ttotal: 35.4s\tremaining: 19.5s\n",
      "129:\tlearn: 0.3697600\ttotal: 35.7s\tremaining: 19.2s\n",
      "130:\tlearn: 0.3664408\ttotal: 35.9s\tremaining: 18.9s\n",
      "131:\tlearn: 0.3643238\ttotal: 36.2s\tremaining: 18.6s\n",
      "132:\tlearn: 0.3625515\ttotal: 36.4s\tremaining: 18.3s\n",
      "133:\tlearn: 0.3600562\ttotal: 36.7s\tremaining: 18.1s\n",
      "134:\tlearn: 0.3576507\ttotal: 36.9s\tremaining: 17.8s\n",
      "135:\tlearn: 0.3555734\ttotal: 37.2s\tremaining: 17.5s\n",
      "136:\tlearn: 0.3540244\ttotal: 37.4s\tremaining: 17.2s\n",
      "137:\tlearn: 0.3512442\ttotal: 37.6s\tremaining: 16.9s\n",
      "138:\tlearn: 0.3497107\ttotal: 37.9s\tremaining: 16.6s\n",
      "139:\tlearn: 0.3478471\ttotal: 38.1s\tremaining: 16.3s\n",
      "140:\tlearn: 0.3467924\ttotal: 38.4s\tremaining: 16.1s\n",
      "141:\tlearn: 0.3454972\ttotal: 38.6s\tremaining: 15.8s\n",
      "142:\tlearn: 0.3442714\ttotal: 38.9s\tremaining: 15.5s\n",
      "143:\tlearn: 0.3416716\ttotal: 39.2s\tremaining: 15.2s\n",
      "144:\tlearn: 0.3404158\ttotal: 39.6s\tremaining: 15s\n",
      "145:\tlearn: 0.3389484\ttotal: 39.9s\tremaining: 14.8s\n",
      "146:\tlearn: 0.3374446\ttotal: 40.2s\tremaining: 14.5s\n",
      "147:\tlearn: 0.3357973\ttotal: 40.4s\tremaining: 14.2s\n",
      "148:\tlearn: 0.3343159\ttotal: 40.7s\tremaining: 13.9s\n",
      "149:\tlearn: 0.3333106\ttotal: 40.9s\tremaining: 13.6s\n",
      "150:\tlearn: 0.3314407\ttotal: 41.2s\tremaining: 13.4s\n",
      "151:\tlearn: 0.3301737\ttotal: 41.5s\tremaining: 13.1s\n",
      "152:\tlearn: 0.3285779\ttotal: 41.8s\tremaining: 12.8s\n",
      "153:\tlearn: 0.3272865\ttotal: 42.1s\tremaining: 12.6s\n",
      "154:\tlearn: 0.3248029\ttotal: 42.5s\tremaining: 12.3s\n",
      "155:\tlearn: 0.3232490\ttotal: 42.8s\tremaining: 12.1s\n",
      "156:\tlearn: 0.3220858\ttotal: 43s\tremaining: 11.8s\n",
      "157:\tlearn: 0.3204571\ttotal: 43.3s\tremaining: 11.5s\n",
      "158:\tlearn: 0.3191269\ttotal: 43.7s\tremaining: 11.3s\n",
      "159:\tlearn: 0.3177266\ttotal: 44s\tremaining: 11s\n",
      "160:\tlearn: 0.3165391\ttotal: 44.4s\tremaining: 10.8s\n",
      "161:\tlearn: 0.3149635\ttotal: 44.7s\tremaining: 10.5s\n",
      "162:\tlearn: 0.3129803\ttotal: 45.1s\tremaining: 10.2s\n",
      "163:\tlearn: 0.3119928\ttotal: 45.3s\tremaining: 9.95s\n",
      "164:\tlearn: 0.3103510\ttotal: 45.7s\tremaining: 9.69s\n",
      "165:\tlearn: 0.3092436\ttotal: 45.9s\tremaining: 9.4s\n",
      "166:\tlearn: 0.3072057\ttotal: 46.2s\tremaining: 9.12s\n",
      "167:\tlearn: 0.3060966\ttotal: 46.5s\tremaining: 8.85s\n",
      "168:\tlearn: 0.3045186\ttotal: 46.7s\tremaining: 8.57s\n",
      "169:\tlearn: 0.3025900\ttotal: 46.9s\tremaining: 8.28s\n",
      "170:\tlearn: 0.3009296\ttotal: 47.2s\tremaining: 8s\n",
      "171:\tlearn: 0.2995697\ttotal: 47.4s\tremaining: 7.72s\n",
      "172:\tlearn: 0.2982727\ttotal: 47.7s\tremaining: 7.44s\n",
      "173:\tlearn: 0.2974583\ttotal: 47.9s\tremaining: 7.16s\n",
      "174:\tlearn: 0.2955430\ttotal: 48.1s\tremaining: 6.88s\n",
      "175:\tlearn: 0.2937187\ttotal: 48.4s\tremaining: 6.6s\n",
      "176:\tlearn: 0.2927679\ttotal: 48.7s\tremaining: 6.33s\n",
      "177:\tlearn: 0.2915619\ttotal: 49s\tremaining: 6.05s\n",
      "178:\tlearn: 0.2903045\ttotal: 49.2s\tremaining: 5.78s\n",
      "179:\tlearn: 0.2885892\ttotal: 49.6s\tremaining: 5.51s\n",
      "180:\tlearn: 0.2877196\ttotal: 50s\tremaining: 5.25s\n",
      "181:\tlearn: 0.2857875\ttotal: 50.3s\tremaining: 4.98s\n",
      "182:\tlearn: 0.2847360\ttotal: 50.7s\tremaining: 4.71s\n",
      "183:\tlearn: 0.2833932\ttotal: 51s\tremaining: 4.44s\n",
      "184:\tlearn: 0.2820561\ttotal: 51.3s\tremaining: 4.16s\n",
      "185:\tlearn: 0.2809870\ttotal: 51.6s\tremaining: 3.88s\n",
      "186:\tlearn: 0.2793906\ttotal: 51.8s\tremaining: 3.6s\n",
      "187:\tlearn: 0.2785644\ttotal: 52.1s\tremaining: 3.32s\n",
      "188:\tlearn: 0.2770691\ttotal: 52.3s\tremaining: 3.04s\n",
      "189:\tlearn: 0.2758853\ttotal: 52.6s\tremaining: 2.77s\n",
      "190:\tlearn: 0.2749348\ttotal: 52.8s\tremaining: 2.49s\n",
      "191:\tlearn: 0.2735451\ttotal: 53.1s\tremaining: 2.21s\n",
      "192:\tlearn: 0.2728460\ttotal: 53.3s\tremaining: 1.93s\n",
      "193:\tlearn: 0.2715508\ttotal: 53.6s\tremaining: 1.66s\n",
      "194:\tlearn: 0.2706133\ttotal: 53.8s\tremaining: 1.38s\n",
      "195:\tlearn: 0.2692027\ttotal: 54.1s\tremaining: 1.1s\n",
      "196:\tlearn: 0.2679456\ttotal: 54.3s\tremaining: 827ms\n",
      "197:\tlearn: 0.2667117\ttotal: 54.5s\tremaining: 551ms\n",
      "198:\tlearn: 0.2654235\ttotal: 54.8s\tremaining: 275ms\n",
      "199:\tlearn: 0.2645252\ttotal: 55.1s\tremaining: 0us\n",
      "roc auc:  0.7995633397135891\n",
      "error:  0.20019833891161523\n",
      "f1:  0.8135749740274731\n",
      "precision : 0.7634315424610052\n",
      "recall : 0.8707684704719545\n",
      "***********************\n",
      " predicted False  predicted True              \n",
      "            2928            1092  actual False\n",
      "             523            3524   actual True\n"
     ]
    }
   ],
   "source": [
    "model2 = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.03,\n",
    "    random_seed=63\n",
    ")\n",
    "\n",
    "model2.fit(\n",
    "    [np.append(np.append(t[0], t[1]), t[2]) for t in train_features_unseen], train_labels_unseen, init_model=model,\n",
    "#     cat_features=cat_features,\n",
    "#     eval_set=(X_validation, y_validation),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictions = model2.predict([np.append(np.append(t[0], t[1]), t[2]) for t in test_features_unseen])\n",
    "errors = abs(predictions - test_labels_unseen)\n",
    "roc_auc = roc_auc_score(test_labels_unseen, predictions)\n",
    "print('roc auc: ', roc_auc)\n",
    "print('error: ', sum(errors)/len(predictions))\n",
    "print('f1: ', f1_score(test_labels_unseen, predictions))\n",
    "print('precision :', precision_score(test_labels_unseen, predictions))\n",
    "print('recall :', recall_score(test_labels_unseen, predictions))  \n",
    "print('***********************')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(test_labels_unseen, predictions))\n",
    "confusion_df.columns = ['predicted False', 'predicted True']\n",
    "confusion_df[''] = ['actual False', 'actual True']\n",
    "print(confusion_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc:  0.7742804694098079\n",
      "error:  0.22523862650303705\n",
      "f1:  0.8035038390829459\n",
      "precision : 0.7144230769230769\n",
      "recall : 0.9179639238942426\n",
      "***********************\n",
      " predicted False  predicted True              \n",
      "            2535            1485  actual False\n",
      "             332            3715   actual True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "train_set = [np.append(np.append(t[0], t[1]), t[2]) for t in train_features_combined]\n",
    "rf.fit(train_set, train_labels_combined)\n",
    "\n",
    "predictions = rf.predict([np.append(np.append(t[0], t[1]), t[2]) for t in test_features_unseen])\n",
    "predictions_probs = rf.predict_proba([np.append(np.append(t[0], t[1]), t[2]) for t in test_features_unseen])\n",
    "errors = abs(predictions - test_labels_unseen)\n",
    "\n",
    "roc_auc = roc_auc_score(test_labels_unseen, predictions)\n",
    "print('roc auc: ', roc_auc)\n",
    "print('error: ', sum(errors)/len(predictions))\n",
    "print('f1: ', f1_score(test_labels_unseen, predictions))\n",
    "print('precision :', precision_score(test_labels_unseen, predictions))\n",
    "print('recall :', recall_score(test_labels_unseen, predictions))  \n",
    "print('***********************')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(test_labels_unseen, predictions))\n",
    "confusion_df.columns = ['predicted False', 'predicted True']\n",
    "confusion_df[''] = ['actual False', 'actual True']\n",
    "print(confusion_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including Trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "train_set = [np.append(np.append(t[0], t[1]), t[2]) for t in train_features]\n",
    "rf.fit(train_set, train_labels)\n",
    "\n",
    "predictions = rf.predict([np.append(np.append(t[0], t[1]), t[2]) for t in test_features])\n",
    "predictions_probs = rf.predict_proba([np.append(np.append(t[0], t[1]), t[2]) for t in test_features])\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "roc_auc = roc_auc_score(test_labels, predictions)\n",
    "print('roc auc: ', roc_auc)\n",
    "print('error: ', sum(errors)/len(predictions))\n",
    "print('f1: ', f1_score(test_labels, predictions))\n",
    "print('precision :', precision_score(test_labels, predictions))\n",
    "print('recall :', recall_score(test_labels, predictions))  \n",
    "print('***********************')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(test_labels, predictions))\n",
    "confusion_df.columns = ['predicted False', 'predicted True']\n",
    "confusion_df[''] = ['actual False', 'actual True']\n",
    "print(confusion_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016275980861575848"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc:  0.6827797649463464\n",
      "error:  0.32323232323232326\n",
      "f1:  0.7117117117117117\n",
      "precision : 0.6220472440944882\n",
      "recall : 0.8315789473684211\n",
      "***********************\n",
      " predicted False  predicted True              \n",
      "              55              48  actual False\n",
      "              16              79   actual True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100)\n",
    "train_set = [np.append(np.append(t[0], t[1]), t[2]) for t in train_features]\n",
    "rf.fit(train_set, train_labels)\n",
    "\n",
    "test_features_not_trivial = []\n",
    "test_labels_not_trivial = []\n",
    "for i, t in enumerate(test_features):\n",
    "    if (t[3] == t[4] and test_labels[i] == 0) or (t[3] != t[4] and test_labels[i] == 1):\n",
    "        test_features_not_trivial.append(t)\n",
    "        test_labels_not_trivial.append(test_labels[i])\n",
    "\n",
    "predictions = rf.predict([np.append(np.append(t[0], t[1]), t[2]) for t in test_features_not_trivial])\n",
    "predictions_probs = rf.predict_proba([np.append(np.append(t[0], t[1]), t[2]) for t in test_features_not_trivial])\n",
    "errors = abs(predictions - test_labels_not_trivial)\n",
    "\n",
    "roc_auc = roc_auc_score(test_labels_not_trivial, predictions)\n",
    "print('roc auc: ', roc_auc)\n",
    "print('error: ', sum(errors)/len(predictions))\n",
    "print('f1: ', f1_score(test_labels_not_trivial, predictions))\n",
    "print('precision :', precision_score(test_labels_not_trivial, predictions))\n",
    "print('recall :', recall_score(test_labels_not_trivial, predictions))  \n",
    "print('***********************')\n",
    "confusion_df = pd.DataFrame(confusion_matrix(test_labels_not_trivial, predictions))\n",
    "confusion_df.columns = ['predicted False', 'predicted True']\n",
    "confusion_df[''] = ['actual False', 'actual True']\n",
    "print(confusion_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n",
      "205\n",
      "233\n",
      "559\n",
      "571\n",
      "660\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "wrong_list = []\n",
    "for pred, label in zip(predictions, test_labels):\n",
    "    if pred != label:\n",
    "        print(index)\n",
    "        wrong_list.append(index)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  0  prob:  [0.5 0.5]\n",
      "label:  1\n",
      "JT   Thompson\n",
      "************************************\n",
      "pred:  0  prob:  [0.5 0.5]\n",
      "label:  1\n",
      "Caldwell   William Caldwell III\n",
      "************************************\n",
      "pred:  0  prob:  [0.54 0.46]\n",
      "label:  1\n",
      "Lebanese Republic   lebanon\n",
      "************************************\n",
      "pred:  0  prob:  [0.51 0.49]\n",
      "label:  1\n",
      "Sea   SEAHAWKS\n",
      "************************************\n",
      "pred:  0  prob:  [0.5 0.5]\n",
      "label:  1\n",
      "Vikes   Minnesota\n",
      "************************************\n",
      "pred:  0  prob:  [0.53 0.47]\n",
      "label:  0\n",
      "PCE   PCE\n",
      "************************************\n",
      "pred:  1  prob:  [0.48 0.52]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-dea19594a4ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     if (train_features[i][1] != train_features[i][2] and test_labels[i] == 1) or (train_features[i][1] == train_features[i][2] and test_labels[i] == 0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pred: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' prob: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_not_trivial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features_not_trivial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features_not_trivial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'************************************'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in wrong_list:\n",
    "#     if (train_features[i][1] != train_features[i][2] and test_labels[i] == 1) or (train_features[i][1] == train_features[i][2] and test_labels[i] == 0):\n",
    "    print('pred: ', predictions[i], ' prob: ', predictions_probs[i])\n",
    "    print('label: ', test_labels_not_trivial[i])\n",
    "    print(test_features_not_trivial[i][1], ' ', test_features_not_trivial[i][2])\n",
    "    print('************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('pairs_classifier_zero_shot.pickle', 'wb') as handle:\n",
    "with open('pairs_classifier_tac_2_10_20.pickle', 'wb') as handle:\n",
    "    pickle.dump(rf, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0., 0., 0., ..., 0., 0., 0.]), 1, 2, 'vegas', 'Vegas',\n",
       "       'bolt-eng-DF-200-192434-4518056', 'bolt-eng-DF-201-185517-399629'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_thesis_venv",
   "language": "python",
   "name": "my_thesis_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
